# modeltextimage.py
import streamlit as st
from transformers import T5Tokenizer, T5ForConditionalGeneration
import torch

@st.cache_resource
def _load_summarizer():
    model_name = "IlyaGusev/ru_t5_base_summarizer"
    tokenizer = T5Tokenizer.from_pretrained(model_name)
    model = T5ForConditionalGeneration.from_pretrained(model_name)
    return tokenizer, model

def show_page():
    st.title("‚úÇÔ∏è –°—É–º–º–∞—Ä–∏–∑–∞—Ü–∏—è —Ç–µ–∫—Å—Ç–∞ –Ω–∞ —Ä—É—Å—Å–∫–æ–º")
    st.caption("–í–≤–µ–¥–∏—Ç–µ –ª—é–±–æ–π —Ç–µ–∫—Å—Ç –Ω–∞ —Ä—É—Å—Å–∫–æ–º —è–∑—ã–∫–µ ‚Äî –º–æ–¥–µ–ª—å —Å–æ–∑–¥–∞—Å—Ç –∫—Ä–∞—Ç–∫–æ–µ —Å–æ–¥–µ—Ä–∂–∞–Ω–∏–µ.")

    input_text = st.text_area(
        "üìù –¢–µ–∫—Å—Ç –¥–ª—è —Å—É–º–º–∞—Ä–∏–∑–∞—Ü–∏–∏:",
        "–ò—Å–∫—É—Å—Å—Ç–≤–µ–Ω–Ω—ã–π –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç ‚Äî —ç—Ç–æ –æ–±–ª–∞—Å—Ç—å –∫–æ–º–ø—å—é—Ç–µ—Ä–Ω—ã—Ö –Ω–∞—É–∫, –∫–æ—Ç–æ—Ä–∞—è –∑–∞–Ω–∏–º–∞–µ—Ç—Å—è —Å–æ–∑–¥–∞–Ω–∏–µ–º —Å–∏—Å—Ç–µ–º, —Å–ø–æ—Å–æ–±–Ω—ã—Ö –≤—ã–ø–æ–ª–Ω—è—Ç—å –∑–∞–¥–∞—á–∏, —Ç—Ä–µ–±—É—é—â–∏–µ —á–µ–ª–æ–≤–µ—á–µ—Å–∫–æ–≥–æ –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç–∞. –ö —Ç–∞–∫–∏–º –∑–∞–¥–∞—á–∞–º –æ—Ç–Ω–æ—Å—è—Ç—Å—è —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏–µ —Ä–µ—á–∏, –ø—Ä–∏–Ω—è—Ç–∏–µ —Ä–µ—à–µ–Ω–∏–π, –æ–±—É—á–µ–Ω–∏–µ –∏ –ø–æ–Ω–∏–º–∞–Ω–∏–µ –µ—Å—Ç–µ—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ —è–∑—ã–∫–∞.",
        height=200
    )

    if st.button("–°–æ–∑–¥–∞—Ç—å –∫—Ä–∞—Ç–∫–æ–µ —Å–æ–¥–µ—Ä–∂–∞–Ω–∏–µ"):
        if not input_text.strip():
            st.warning("–ü–æ–∂–∞–ª—É–π—Å—Ç–∞, –≤–≤–µ–¥–∏—Ç–µ —Ç–µ–∫—Å—Ç.")
            return

        try:
            with st.spinner("–ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Å—É–º–º–∞—Ä–∏–∑–∞—Ü–∏–∏... (–º–æ–∂–µ—Ç –∑–∞–Ω—è—Ç—å –¥–æ 1 –º–∏–Ω—É—Ç—ã)"):
                tokenizer, model = _load_summarizer()
                inputs = tokenizer(
                    input_text,
                    return_tensors="pt",
                    max_length=1024,
                    truncation=True,
                    padding="max_length"
                )
                summary_ids = model.generate(
                    inputs.input_ids,
                    max_length=256,
                    min_length=30,
                    length_penalty=1.2,
                    num_beams=4,
                    early_stopping=True
                )
                summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)

            st.subheader("‚úÖ –†–µ–∑—É–ª—å—Ç–∞—Ç:")
            st.write(summary)
        except Exception as e:
            st.error(f"–û—à–∏–±–∫–∞: {str(e)}")
